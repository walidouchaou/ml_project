{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: File is already open in \nC:\\Users\\ouchaou\\AppData\\Local\\Microsoft\\WinGet\\Packages\\DuckDB.cli_Microsoft.Winget.Source_8wekyb3d8bbwe\\duckdb.exe (PID 15756)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIOException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Étape 1 : Chargement des données et nettoyage\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m con = \u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/ouchaou/Desktop/Project_data_portfolio/data_pipeline_streaming_finnhub/src/duckdb/data/trades.duckdb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mselect * from trades_with_indicators_view where datetime > \u001b[39m\u001b[33m'\u001b[39m\u001b[33m2024-03-01 00:10:00\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and datetime < \u001b[39m\u001b[33m'\u001b[39m\u001b[33m2025-03-07 00:15:00\u001b[39m\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m df = con.execute(query).fetchdf()\n",
      "\u001b[31mIOException\u001b[39m: IO Error: File is already open in \nC:\\Users\\ouchaou\\AppData\\Local\\Microsoft\\WinGet\\Packages\\DuckDB.cli_Microsoft.Winget.Source_8wekyb3d8bbwe\\duckdb.exe (PID 15756)"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 1 : Chargement des données et nettoyage\n",
    "# ----------------------------------------------------------------\n",
    "con = duckdb.connect(\"C:/Users/ouchaou/Desktop/Project_data_portfolio/data_pipeline_streaming_finnhub/src/duckdb/data/trades.duckdb\")\n",
    "query = \"select * from trades_with_indicators_view where datetime > '2024-03-01 00:10:00' ;\"\n",
    "con.close()\n",
    "\n",
    "# Suppression des lignes avec valeurs manquantes\n",
    "df = df.dropna()\n",
    "print(f\"Nombre de valeurs NaN dans le DataFrame: {df.isna().sum().sum()}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 2 : Création de la cible et sélection des features\n",
    "# ----------------------------------------------------------------\n",
    "df['target'] = df['close_price'].shift(-1)\n",
    "df = df.dropna()  # Supprimer les lignes NaN après le shift\n",
    "\n",
    "features = [\n",
    "    'BB_upper', 'min_price', 'max_price', 'EMA_50', 'open_price'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 3 : Division train/test et normalisation\n",
    "# ----------------------------------------------------------------\n",
    "# Division des données en ensembles d'entraînement (80%) et de test (20%)\n",
    "# test_size=0.2 signifie que 20% des données sont réservées pour le test\n",
    "# shuffle=False conserve l'ordre chronologique des données, important pour les séries temporelles\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 4 : Validation avec TimeSeriesSplit\n",
    "# ----------------------------------------------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "cv_rmse = []  # Stocker les erreurs pour chaque split\n",
    "\n",
    "for train_index, val_index in tscv.split(X_train_scaled):\n",
    "    X_tr, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=178,\n",
    "        learning_rate=0.045,\n",
    "        max_depth=3,\n",
    "        subsample=0.85,\n",
    "        random_state=42,\n",
    "        n_iter_no_change=10,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Validation\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "    cv_rmse.append(rmse_val)\n",
    "\n",
    "# Afficher la moyenne des RMSE sur les folds\n",
    "print(f\"Validation RMSE moyenne sur TimeSeriesSplit : {np.mean(cv_rmse):.2f}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 5 : Entraînement du modèle final avec toutes les données d'entraînement\n",
    "# ----------------------------------------------------------------\n",
    "best_model = GradientBoostingRegressor(\n",
    "    n_estimators=178,\n",
    "    learning_rate=0.045,\n",
    "    max_depth=3,\n",
    "    subsample=0.85,\n",
    "    random_state=42,\n",
    "    n_iter_no_change=10,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 6 : Évaluation finale sur test set\n",
    "# ----------------------------------------------------------------\n",
    "y_pred_train = best_model.predict(X_train_scaled)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(f\"RMSE sur données d'entraînement: {rmse_train}\")\n",
    "\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(f\"RMSE sur données de test: {rmse_test}\")\n",
    "\n",
    "# 1) MAE, MAPE, R²\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"MAE sur données de test: {mae_test}\")\n",
    "print(f\"MAPE sur données de test: {mape_test:.2f}%\")\n",
    "print(f\"R² sur données de test: {r2_test}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 7 : Visualisation des erreurs et prédictions\n",
    "# ----------------------------------------------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.reset_index(drop=True), label='Valeurs Réelles')\n",
    "plt.plot(y_pred_test, label='Prédictions')\n",
    "plt.legend()\n",
    "plt.title(\"Comparaison entre valeurs réelles et prédictions\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Prix de clôture\")\n",
    "plt.show()\n",
    "\n",
    "# Importance des features\n",
    "importances = best_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'feature': features, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "print(feature_importance_df)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_importance_df['feature'], feature_importance_df['importance'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Importance des caractéristiques\")\n",
    "plt.xlabel(\"Caractéristiques\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Étape 8 : Enregistrement du modèle final\n",
    "# ----------------------------------------------------------------\n",
    "joblib.dump(best_model, 'gradient_boosting_model.pkl')\n",
    "print(\"Modèle enregistré sous 'gradient_boosting_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mpredicted_close\u001b[39m\u001b[33m\"\u001b[39m: prediction}\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[43muvicorn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouchaou\\Desktop\\Project_data_portfolio\\data_pipeline_streaming_finnhub\\finhub_test\\Lib\\site-packages\\uvicorn\\main.py:579\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[39m\n\u001b[32m    577\u001b[39m         Multiprocess(config, target=server.run, sockets=[sock]).run()\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m         \u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# pragma: full coverage\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouchaou\\Desktop\\Project_data_portfolio\\data_pipeline_streaming_finnhub\\finhub_test\\Lib\\site-packages\\uvicorn\\server.py:66\u001b[39m, in \u001b[36mServer.run\u001b[39m\u001b[34m(self, sockets)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket.socket] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.setup_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[43m=\u001b[49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Chargement du modèle et du scaler sauvegardés\n",
    "model = joblib.load(\"gradient_boosting_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")  # Assure-toi de sauvegarder également le scaler\n",
    "\n",
    "features = [\n",
    "    'open_price', 'close_price', 'max_price', 'min_price', 'total_volume',\n",
    "    'price_range', 'volatility_pct', 'SMA_50', 'EMA_50', 'BB_upper', \n",
    "    'BB_lower', 'RSI', 'MACD', 'MACD_signal', 'volatility_7h', 'volatility_30h',\n",
    "    'hour', 'day_of_week', 'month',\n",
    "    'close_price_lag_1', 'close_price_lag_2', 'close_price_lag_3'\n",
    "]\n",
    "features = [\n",
    "    'bb_upper', 'min_price', 'max_price', 'ema_50','max_price','open_price']\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "def predict(\n",
    "    open_price: float,\n",
    "    close_price: float,\n",
    "    max_price: float,\n",
    "    min_price: float,\n",
    "    EMA_50: float,\n",
    "    BB_upper: float,t\n",
    "):\n",
    "    # Créer un DataFrame avec les features\n",
    "    data = {\n",
    "        'open_price': [open_price],\n",
    "        'close_price': [close_price],\n",
    "        'max_price': [max_price],\n",
    "        'min_price': [min_price],\n",
    "        'EMA_50': [EMA_50],\n",
    "        'BB_upper': [BB_upper]\n",
    "    }\n",
    "    df_features = pd.DataFrame(data, columns=features)\n",
    "    \n",
    "    # Mise à l'échelle\n",
    "    df_scaled = scaler.transform(df_features)\n",
    "    \n",
    "    # Prédiction\n",
    "    prediction = model.predict(df_scaled)[0]\n",
    "    return {\"predicted_close\": prediction}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finhub_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
